{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdLWfJJVVqCq3xYyRNao0e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yhpark1962/baseball/blob/main/news_crawling_Best.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMWPAxxy2Eph",
        "outputId": "47a43465-52d9-4105-bf9f-dbdb446f202e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "keyword to search: 양자 컴퓨터\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 설치 및 가져오기\n",
        "!pip install requests\n",
        "!pip install pandas\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# 검색어 입력\n",
        "query = input(\"keyword to search: \")\n",
        "\n",
        "# Google 뉴스 검색 URL (start 파라미터로 페이지 변경)\n",
        "base_url = f\"https://www.google.com/search?q={query}&tbm=nws\"\n",
        "\n",
        "# HTTP 요청 헤더 설정\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\n",
        "    \"Accept-Language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
        "}\n",
        "\n",
        "# 뉴스 데이터 추출\n",
        "titles = []\n",
        "contents = []\n",
        "links = []\n",
        "dates = []\n",
        "\n",
        "# 원하는 뉴스 갯수 설정\n",
        "max_news = 40\n",
        "page = 0\n",
        "\n",
        "while len(titles) < max_news:\n",
        "    # start 파라미터로 여러 페이지 크롤링\n",
        "    url = f\"{base_url}&start={page * 10}\"\n",
        "\n",
        "    # HTTP GET 요청 보내기\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = bs(response.text, \"html.parser\")\n",
        "\n",
        "    # 뉴스 아이템 선택\n",
        "    news_items = soup.select(\".SoaBEf\")\n",
        "\n",
        "    for item in news_items:\n",
        "        title = item.select_one(\".nDgy9d\").text if item.select_one(\".nDgy9d\") else \"제목 없음\"\n",
        "        content = item.select_one(\".Y3v8qd\").text if item.select_one(\".Y3v8qd\") else \"본문 없음\"\n",
        "        link = item.select_one(\"a\").get(\"href\") if item.select_one(\"a\") else \"링크 없음\"\n",
        "\n",
        "       # 날짜 추출 부분 수정\n",
        "date_text = item.select_one(\".WG9SHc\")\n",
        "if date_text:\n",
        "    date_text = date_text.text.strip()\n",
        "else:\n",
        "    date_text = None\n",
        "\n",
        "# 날짜가 없으면 다른 방법으로 추출 (구글 뉴스의 형식에 따라 다를 수 있음)\n",
        "if date_text is None:\n",
        "    date_text = item.select_one(\".rc .slp .MUxGbd\")  # 날짜가 있을 수 있는 다른 태그 찾기\n",
        "    if date_text:\n",
        "        date_text = date_text.text.strip()\n",
        "\n",
        "# 출력해봐서 어떤 형식으로 날짜가 추출되는지 확인\n",
        "print(\"Date text:\", date_text)\n",
        "\n",
        "# 날짜 형식이 '오늘'이면 오늘 뉴스로 분류\n",
        "if date_text and \"오늘\" in date_text:\n",
        "    news_date = datetime.today().date()\n",
        "else:\n",
        "    if date_text:  # date_text가 None이 아닐 경우에만 처리\n",
        "        try:\n",
        "            # 날짜가 '오늘'이 아니면 다양한 형식 (2025.03.19, 2025-03-19, 2025/03/19)을 처리\n",
        "            news_date = datetime.strptime(date_text, \"%Y.%m.%d\").date()  # 2025.03.19 형식\n",
        "        except ValueError:\n",
        "            try:\n",
        "                news_date = datetime.strptime(date_text, \"%Y-%m-%d\").date()  # 2025-03-19 형식\n",
        "            except ValueError:\n",
        "                try:\n",
        "                    news_date = datetime.strptime(date_text, \"%Y/%m/%d\").date()  # 2025/03/19 형식\n",
        "                except ValueError:\n",
        "                    news_date = None\n",
        "    else:\n",
        "        news_date = None\n",
        "\n",
        "\n",
        "        # 제목, 본문, 링크, 날짜 추가\n",
        "        titles.append(title)\n",
        "        contents.append(content)\n",
        "        links.append(link)\n",
        "        dates.append(news_date)\n",
        "\n",
        "        # 최대 뉴스 갯수에 도달하면 종료\n",
        "        if len(titles) >= max_news:\n",
        "            break\n",
        "\n",
        "    page += 1  # 다음 페이지로 이동\n",
        "\n",
        "# 데이터프레임 생성\n",
        "news_df = pd.DataFrame({\n",
        "    \"Titles\": titles,\n",
        "    \"Contents\": contents,\n",
        "    \"Links\": links,\n",
        "    \"Date\": dates\n",
        "})\n",
        "\n",
        "# 최신 뉴스가 먼저 오도록 날짜 기준으로 정렬\n",
        "news_df = news_df.sort_values(by=\"Date\", ascending=False)\n",
        "\n",
        "# Colab 환경에서 파일 저장 경로 설정\n",
        "output_path = \"/content/news_crawling.csv\"\n",
        "\n",
        "# CSV로 저장\n",
        "news_df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(f\"news data saved: {output_path}\")\n",
        "\n",
        "# 데이터 미리보기\n",
        "news_df.head()\n"
      ]
    }
  ]
}